{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T09:33:52.395609Z",
     "start_time": "2025-09-21T09:33:52.386661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from plotly.express.trendline_functions import rolling\n",
    "\n"
   ],
   "id": "7ac93eb1c367b7ad",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T09:33:57.340003Z",
     "start_time": "2025-09-21T09:33:52.414956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LOAD the DATASET from the server\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import getpass\n",
    "import itertools\n",
    "\n",
    "password = getpass.getpass(\"Database password: \")\n",
    "engine = create_engine(f\"postgresql://postgres:{password}@localhost:5432/fintech_db\")\n",
    "\n",
    "# Load from PostgreSQL server\n",
    "df_loaded = pd.read_sql(\"SELECT * FROM my_portfolio\", engine)\n",
    "\n",
    "df = df_loaded  # rename\n",
    "\n",
    "engine.dispose()\n",
    "\n",
    "#print(df.head())\n",
    "# ŒöœÅŒ±œÑŒ¨œÇ ŒºœåŒΩŒø œÑŒπœÇ œÉœÑŒÆŒªŒµœÇ œÄŒøœÖ œáœÅŒµŒπŒ¨Œ∂ŒµœÉŒ±Œπ\n",
    "df = df[['day', 'symbol', 'close']]\n",
    "\n",
    "# Pivot Œ≥ŒπŒ± wide format\n",
    "df_wide = df.pivot(index='day', columns='symbol', values='close')\n",
    "df_wide.columns.name = None  # Œ±œÜŒ±ŒπœÅŒµŒØ œÑŒø œåŒΩŒøŒºŒ± 'symbol'\n",
    "df_wide.index.name = None\n",
    "\n",
    "# ŒúŒµœÑŒ±œÑœÅŒ≠œÄŒøœÖŒºŒµ œÑŒø index œÉŒµ datetime\n",
    "df_wide.index = pd.to_datetime(df_wide.index)\n",
    "\n",
    "df_portfolio = df_wide\n",
    "\n",
    "df_original = df_portfolio[['BTC', 'ETH']]\n",
    "df_missing = df_portfolio[['BTC', 'ETH']]\n",
    "\n",
    "# ŒíŒ¨Œ∂ŒøœÖŒºŒµ NaN ŒºœåŒΩŒø Œ≥ŒπŒ± Œ£Œ±Œ≤Œ≤Œ±œÑŒøŒ∫œçœÅŒπŒ±Œ∫Œ±\n",
    "df_missing.loc[df_portfolio.index.weekday >= 5] = np.nan\n",
    "\n",
    "#print(df_original)\n",
    "print(df_missing)"
   ],
   "id": "afed6ba0cd34982d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  BTC      ETH\n",
      "2015-01-01  154188.23   576.13\n",
      "2015-01-02  147790.81   596.53\n",
      "2015-01-03        NaN      NaN\n",
      "2015-01-04        NaN      NaN\n",
      "2015-01-05  167029.67   605.92\n",
      "...               ...      ...\n",
      "2024-12-27   52541.97  2580.50\n",
      "2024-12-28        NaN      NaN\n",
      "2024-12-29        NaN      NaN\n",
      "2024-12-30   57827.88  2485.35\n",
      "2024-12-31   57125.50  2603.72\n",
      "\n",
      "[3653 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T09:33:57.467134Z",
     "start_time": "2025-09-21T09:33:57.367216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class FinancialImputationAnalyzer:\n",
    "    \"\"\"\n",
    "    Advanced imputation analyzer specifically designed for financial time series data.\n",
    "    Implements mean/median imputation with financial data considerations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_with_missing, original_data=None):\n",
    "        \"\"\"\n",
    "        Initialize the imputation analyzer\n",
    "\n",
    "        Parameters:\n",
    "        - data_with_missing: DataFrame or numpy array with missing values\n",
    "        - original_data: Original complete data for evaluation (optional)\n",
    "        \"\"\"\n",
    "        self.data_missing = data_with_missing.copy() if hasattr(data_with_missing, 'copy') else data_with_missing.copy()\n",
    "        self.original_data = original_data.copy() if original_data is not None else None\n",
    "        self.imputation_results = {}\n",
    "        self.performance_metrics = {}\n",
    "\n",
    "    def analyze_missing_pattern(self):\n",
    "        \"\"\"Analyze the pattern of missing values for financial context\"\"\"\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            missing_info = self.data_missing.isnull()\n",
    "        else:\n",
    "            missing_info = pd.DataFrame(np.isnan(self.data_missing))\n",
    "\n",
    "        print(\"üîç MISSING VALUE PATTERN ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Overall statistics\n",
    "        total_missing = missing_info.sum().sum()\n",
    "        total_cells = missing_info.shape[0] * missing_info.shape[1]\n",
    "        missing_pct = (total_missing / total_cells) * 100\n",
    "\n",
    "        print(f\"Dataset shape: {missing_info.shape}\")\n",
    "        print(f\"Total missing values: {total_missing:,}\")\n",
    "        print(f\"Missing percentage: {missing_pct:.2f}%\")\n",
    "\n",
    "        # Missing by time period (rows)\n",
    "        missing_by_row = missing_info.sum(axis=1)\n",
    "        print(f\"\\nMissing values per time period:\")\n",
    "        print(f\"  Min: {missing_by_row.min()}\")\n",
    "        print(f\"  Max: {missing_by_row.max()}\")\n",
    "        print(f\"  Mean: {missing_by_row.mean():.1f}\")\n",
    "\n",
    "        # Missing by asset (columns)\n",
    "        missing_by_col = missing_info.sum(axis=0)\n",
    "        print(f\"\\nMissing values per asset:\")\n",
    "        print(f\"  Min: {missing_by_col.min()}\")\n",
    "        print(f\"  Max: {missing_by_col.max()}\")\n",
    "        print(f\"  Mean: {missing_by_col.mean():.1f}\")\n",
    "\n",
    "        # Financial data specific checks\n",
    "        print(f\"\\nüìä FINANCIAL DATA QUALITY CHECKS:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Check for consecutive missing values (problematic for time series)\n",
    "        consecutive_missing = []\n",
    "        for col in range(missing_info.shape[1]):\n",
    "            col_missing = missing_info.iloc[:, col]\n",
    "            consecutive = 0\n",
    "            max_consecutive = 0\n",
    "            for val in col_missing:\n",
    "                if val:\n",
    "                    consecutive += 1\n",
    "                    max_consecutive = max(max_consecutive, consecutive)\n",
    "                else:\n",
    "                    consecutive = 0\n",
    "            consecutive_missing.append(max_consecutive)\n",
    "\n",
    "        max_consecutive_overall = max(consecutive_missing)\n",
    "        print(f\"Maximum consecutive missing values: {max_consecutive_overall}\")\n",
    "\n",
    "        if max_consecutive_overall > 5:\n",
    "            print(\"‚ö†Ô∏è  WARNING: Long consecutive missing periods detected!\")\n",
    "            print(\"   Consider using interpolation instead of mean/median\")\n",
    "        else:\n",
    "            print(\"‚úÖ Missing pattern suitable for mean/median imputation\")\n",
    "\n",
    "        return {\n",
    "            'total_missing': total_missing,\n",
    "            'missing_pct': missing_pct,\n",
    "            'max_consecutive': max_consecutive_overall,\n",
    "            'missing_by_row': missing_by_row,\n",
    "            'missing_by_col': missing_by_col\n",
    "        }\n",
    "\n",
    "    def simple_mean_imputation(self):\n",
    "        \"\"\"\n",
    "        Simple mean imputation - replaces missing values with column mean\n",
    "        ‚ö†Ô∏è WARNING: This distorts variance and ignores time series nature\n",
    "        \"\"\"\n",
    "        print(\"\\nüìä SIMPLE MEAN IMPUTATION\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                mean_val = imputed_data[col].mean()\n",
    "                imputed_data[col].fillna(mean_val, inplace=True)\n",
    "                print(f\"  {col}: filled with mean {mean_val:.2f}\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "                mean_val = np.nanmean(col_data)\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = mean_val\n",
    "                if col < 5:  # Print first 5 for brevity\n",
    "                    print(f\"  Asset {col + 1:03d}: filled with mean {mean_val:.2f}\")\n",
    "\n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1] - 5} more assets\")\n",
    "\n",
    "        self.imputation_results['simple_mean'] = imputed_data\n",
    "        return imputed_data\n",
    "\n",
    "    def simple_median_imputation(self):\n",
    "        \"\"\"\n",
    "        Simple median imputation - replaces missing values with column median\n",
    "        More robust to outliers than mean\n",
    "        \"\"\"\n",
    "        print(\"\\nüìä SIMPLE MEDIAN IMPUTATION\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                median_val = imputed_data[col].median()\n",
    "                imputed_data[col].fillna(median_val, inplace=True)\n",
    "                print(f\"  {col}: filled with median {median_val:.2f}\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "                median_val = np.nanmedian(col_data)\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = median_val\n",
    "                if col < 5:  # Print first 5 for brevity\n",
    "                    print(f\"  Asset {col + 1:03d}: filled with median {median_val:.2f}\")\n",
    "\n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1] - 5} more assets\")\n",
    "\n",
    "        self.imputation_results['simple_median'] = imputed_data\n",
    "        return imputed_data\n",
    "\n",
    "    def rolling_mean_imputation(self, window=30):\n",
    "        \"\"\"\n",
    "        Rolling mean imputation - uses local time window mean\n",
    "        Better for financial time series as it adapts to local trends\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìä ROLLING MEAN IMPUTATION (Window: {window})\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                # Calculate rolling mean\n",
    "                rolling_mean = imputed_data[col].rolling(window=window, center=True, min_periods=1).mean()\n",
    "                # Fill missing values\n",
    "                imputed_data[col] = imputed_data[col].fillna(rolling_mean)\n",
    "                # If still missing (edge cases), use global mean\n",
    "                global_mean = imputed_data[col].mean()\n",
    "                imputed_data[col] = imputed_data[col].fillna(global_mean)\n",
    "\n",
    "                filled_count = self.data_missing[col].isnull().sum()\n",
    "                print(f\"  {col}: filled {filled_count} values with rolling mean\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "\n",
    "                # Create rolling mean using pandas for convenience\n",
    "                temp_series = pd.Series(col_data)\n",
    "                rolling_mean = temp_series.rolling(window=window, center=True, min_periods=1).mean()\n",
    "\n",
    "                # Fill missing values\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = rolling_mean[mask]\n",
    "\n",
    "                # Handle remaining NaN with global mean\n",
    "                remaining_nan = np.isnan(imputed_data[:, col])\n",
    "                if remaining_nan.any():\n",
    "                    global_mean = np.nanmean(imputed_data[:, col])\n",
    "                    imputed_data[remaining_nan, col] = global_mean\n",
    "\n",
    "                if col < 5:\n",
    "                    filled_count = mask.sum()\n",
    "                    print(f\"  Asset {col + 1:03d}: filled {filled_count} values with rolling mean\")\n",
    "\n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1] - 5} more assets\")\n",
    "\n",
    "        self.imputation_results['rolling_mean'] = imputed_data\n",
    "        return imputed_data\n",
    "\n",
    "    def rolling_median_imputation(self, window=30):\n",
    "        \"\"\"\n",
    "        Rolling median imputation - uses local time window median\n",
    "        Even more robust to outliers, good for volatile financial data\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìä ROLLING MEDIAN IMPUTATION (Window: {window})\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        if isinstance(self.data_missing, pd.DataFrame):\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in imputed_data.columns:\n",
    "                # Calculate rolling median\n",
    "                rolling_median = imputed_data[col].rolling(window=window, center=True, min_periods=1).median()\n",
    "                # Fill missing values\n",
    "                imputed_data[col] = imputed_data[col].fillna(rolling_median)\n",
    "                # If still missing (edge cases), use global median\n",
    "                global_median = imputed_data[col].median()\n",
    "                imputed_data[col] = imputed_data[col].fillna(global_median)\n",
    "\n",
    "                filled_count = self.data_missing[col].isnull().sum()\n",
    "                print(f\"  {col}: filled {filled_count} values with rolling median\")\n",
    "        else:\n",
    "            imputed_data = self.data_missing.copy()\n",
    "            for col in range(imputed_data.shape[1]):\n",
    "                col_data = imputed_data[:, col]\n",
    "\n",
    "                # Create rolling median using pandas for convenience\n",
    "                temp_series = pd.Series(col_data)\n",
    "                rolling_median = temp_series.rolling(window=window, center=True, min_periods=1).median()\n",
    "\n",
    "                # Fill missing values\n",
    "                mask = np.isnan(col_data)\n",
    "                imputed_data[mask, col] = rolling_median[mask]\n",
    "\n",
    "                # Handle remaining NaN with global median\n",
    "                remaining_nan = np.isnan(imputed_data[:, col])\n",
    "                if remaining_nan.any():\n",
    "                    global_median = np.nanmedian(imputed_data[:, col])\n",
    "                    imputed_data[remaining_nan, col] = global_median\n",
    "\n",
    "                if col < 5:\n",
    "                    filled_count = mask.sum()\n",
    "                    print(f\"  Asset {col + 1:03d}: filled {filled_count} values with rolling median\")\n",
    "\n",
    "            if imputed_data.shape[1] > 5:\n",
    "                print(f\"  ... and {imputed_data.shape[1] - 5} more assets\")\n",
    "\n",
    "        self.imputation_results['rolling_median'] = imputed_data\n",
    "        return imputed_data\n",
    "\n",
    "    def evaluate_imputation_quality(self, method_name, imputed_data):\n",
    "        \"\"\"\n",
    "        Evaluate imputation quality if original data is available - NO SKLEARN VERSION\n",
    "        \"\"\"\n",
    "        if self.original_data is None:\n",
    "            print(f\"\\n‚ö†Ô∏è  No original data available for {method_name} evaluation\")\n",
    "            return None\n",
    "\n",
    "        print(f\"\\nüìà IMPUTATION QUALITY EVALUATION: {method_name.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            # Force everything to be numpy arrays\n",
    "            if hasattr(self.data_missing, 'values'):\n",
    "                missing_np = self.data_missing.values.copy()\n",
    "            else:\n",
    "                missing_np = self.data_missing.copy()\n",
    "\n",
    "            if hasattr(self.original_data, 'values'):\n",
    "                original_np = self.original_data.values.copy()\n",
    "            else:\n",
    "                original_np = self.original_data.copy()\n",
    "\n",
    "            if hasattr(imputed_data, 'values'):\n",
    "                imputed_np = imputed_data.values.copy()\n",
    "            else:\n",
    "                imputed_np = imputed_data.copy()\n",
    "\n",
    "            print(\n",
    "                f\"  Data shapes - Missing: {missing_np.shape}, Original: {original_np.shape}, Imputed: {imputed_np.shape}\")\n",
    "\n",
    "            # Create mask\n",
    "            mask = np.isnan(missing_np)\n",
    "            total_missing = np.sum(mask)\n",
    "            print(f\"  Found {total_missing} missing values to evaluate\")\n",
    "\n",
    "            if total_missing == 0:\n",
    "                print(\"  No missing values found!\")\n",
    "                return None\n",
    "\n",
    "            # Extract values using simple indexing\n",
    "            orig_vals = []\n",
    "            imp_vals = []\n",
    "\n",
    "            rows, cols = missing_np.shape\n",
    "            for i in range(rows):\n",
    "                for j in range(cols):\n",
    "                    if mask[i, j]:  # This was originally missing\n",
    "                        orig_vals.append(original_np[i, j])\n",
    "                        imp_vals.append(imputed_np[i, j])\n",
    "\n",
    "            orig_vals = np.array(orig_vals)\n",
    "            imp_vals = np.array(imp_vals)\n",
    "\n",
    "            print(f\"  Extracted {len(orig_vals)} value pairs\")\n",
    "\n",
    "            # Simple metrics - MANUAL CALCULATION ONLY\n",
    "            differences = orig_vals - imp_vals\n",
    "            abs_differences = np.abs(differences)\n",
    "\n",
    "            mae = np.mean(abs_differences)\n",
    "            mse = np.mean(differences ** 2)\n",
    "            rmse = np.sqrt(mse)\n",
    "\n",
    "            # Simple percentage error (avoid division by zero)\n",
    "            nonzero_mask = orig_vals != 0\n",
    "            if np.sum(nonzero_mask) > 0:\n",
    "                pct_errors = abs_differences[nonzero_mask] / np.abs(orig_vals[nonzero_mask])\n",
    "                mape = np.mean(pct_errors) * 100\n",
    "            else:\n",
    "                mape = float('inf')\n",
    "\n",
    "            # Simple correlation\n",
    "            if len(orig_vals) > 1:\n",
    "                corr = np.corrcoef(orig_vals, imp_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 1.0\n",
    "\n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "            print(f\"  RMSE: {rmse:.4f}\")\n",
    "            print(f\"  MAPE: {mape:.2f}%\" if np.isfinite(mape) else \"  MAPE: ‚àû\")\n",
    "            print(f\"  Correlation: {corr:.4f}\")\n",
    "\n",
    "            # Financial interpretation\n",
    "            print(f\"\\nüí∞ FINANCIAL INTERPRETATION:\")\n",
    "            print(f\"  Average price difference: ${mae:.2f}\")\n",
    "            print(f\"  Typical error magnitude: ${rmse:.2f}\")\n",
    "\n",
    "            if np.isfinite(mape):\n",
    "                if mape < 5:\n",
    "                    print(\"  ‚úÖ EXCELLENT: Very accurate imputation\")\n",
    "                elif mape < 10:\n",
    "                    print(\"  ‚úÖ GOOD: Acceptable imputation quality\")\n",
    "                elif mape < 20:\n",
    "                    print(\"  ‚ö†Ô∏è  FAIR: Moderate imputation errors\")\n",
    "                else:\n",
    "                    print(\"  ‚ùå POOR: High imputation errors\")\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  Note: MAPE could not be calculated due to zero values\")\n",
    "\n",
    "            metrics = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'Correlation': corr,\n",
    "                'N_Values': len(orig_vals)\n",
    "            }\n",
    "\n",
    "            self.performance_metrics[method_name] = metrics\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR in evaluation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def compare_all_methods(self, window=30):\n",
    "        \"\"\"\n",
    "        Run all imputation methods and compare results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üéØ COMPREHENSIVE MEAN/MEDIAN IMPUTATION ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Analyze missing pattern first\n",
    "        self.analyze_missing_pattern()\n",
    "\n",
    "        # Run all methods\n",
    "        methods = {\n",
    "            'Simple Mean': self.simple_mean_imputation,\n",
    "            'Simple Median': self.simple_median_imputation,\n",
    "            'Rolling Mean': lambda: self.rolling_mean_imputation(window),\n",
    "            'Rolling Median': lambda: self.rolling_median_imputation(window)\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "        for name, method in methods.items():\n",
    "            print(f\"\\n{'=' * 20} {name.upper()} {'=' * 20}\")\n",
    "            imputed = method()\n",
    "            results[name] = imputed\n",
    "\n",
    "            # Evaluate quality\n",
    "            metrics = self.evaluate_imputation_quality(name.lower().replace(' ', '_'), imputed)\n",
    "\n",
    "        # Summary comparison\n",
    "        if self.performance_metrics:\n",
    "            self.print_method_comparison()\n",
    "\n",
    "        return results\n",
    "\n",
    "    def print_method_comparison(self):\n",
    "        \"\"\"Print comparison of all methods\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üèÜ METHOD COMPARISON SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        comparison_df = pd.DataFrame(self.performance_metrics).T\n",
    "        print(comparison_df.round(4))\n",
    "\n",
    "        # Find best method by lowest RMSE\n",
    "        best_method = comparison_df['RMSE'].idxmin()\n",
    "        print(f\"\\nüèÜ BEST PERFORMING METHOD: {best_method.replace('_', ' ').title()}\")\n",
    "        print(f\"   RMSE: {comparison_df.loc[best_method, 'RMSE']:.4f}\")\n",
    "\n",
    "        # Recommendations\n",
    "        print(f\"\\nüí° RECOMMENDATIONS FOR FINANCIAL TIME SERIES:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if 'rolling_median' in self.performance_metrics:\n",
    "            print(\"ü•á BEST PRACTICE: Rolling Median\")\n",
    "            print(\"   ‚úÖ Adapts to local market conditions\")\n",
    "            print(\"   ‚úÖ Robust to price spikes/crashes\")\n",
    "            print(\"   ‚úÖ Preserves time series properties\")\n",
    "\n",
    "        print(f\"\\nü•à ALTERNATIVE: Rolling Mean\")\n",
    "        print(\"   ‚úÖ Good for stable market periods\")\n",
    "        print(\"   ‚ö†Ô∏è  Sensitive to outliers\")\n",
    "\n",
    "        print(f\"\\n‚ö†Ô∏è  AVOID: Simple Mean/Median\")\n",
    "        print(\"   ‚ùå Ignores time series structure\")\n",
    "        print(\"   ‚ùå Can create artificial patterns\")\n",
    "        print(\"   ‚ùå Distorts volatility\")\n",
    "\n",
    "        print(f\"\\nüîÑ NEXT STEPS:\")\n",
    "        print(\"   1. Try interpolation methods (linear, spline)\")\n",
    "        print(\"   2. Consider forward/backward fill\")\n",
    "        print(\"   3. Test advanced methods (KNN, MICE)\")\n",
    "\n",
    "\n",
    "# READY TO USE\n",
    "print(\"=\" * 80)\n",
    "print(\"FINANCIAL IMPUTATION ANALYZER\")\n",
    "print(\"=\" * 80)\n",
    "print(\"analyzer = FinancialImputationAnalyzer(data_with_missing, original_data)\")\n",
    "print(\"results = analyzer.compare_all_methods(window=30)\")# LOAD the DATASET from the server\n"
   ],
   "id": "60e318994710ab57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINANCIAL IMPUTATION ANALYZER\n",
      "================================================================================\n",
      "analyzer = FinancialImputationAnalyzer(data_with_missing, original_data)\n",
      "results = analyzer.compare_all_methods(window=30)\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T09:33:57.505467Z",
     "start_time": "2025-09-21T09:33:57.484751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(f\"Data imputation with simple median  technique\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "analyzer = FinancialImputationAnalyzer(df_missing,df_original)\n",
    "missing_pattern = analyzer.analyze_missing_pattern()\n",
    "simple_median_imputation = analyzer.simple_median_imputation()\n"
   ],
   "id": "3597ed95b7fc9d0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Data imputation with simple median  technique\n",
      "==================================================\n",
      "üîç MISSING VALUE PATTERN ANALYSIS\n",
      "==================================================\n",
      "Dataset shape: (3653, 2)\n",
      "Total missing values: 2,088\n",
      "Missing percentage: 28.58%\n",
      "\n",
      "Missing values per time period:\n",
      "  Min: 0\n",
      "  Max: 2\n",
      "  Mean: 0.6\n",
      "\n",
      "Missing values per asset:\n",
      "  Min: 1044\n",
      "  Max: 1044\n",
      "  Mean: 1044.0\n",
      "\n",
      "üìä FINANCIAL DATA QUALITY CHECKS:\n",
      "----------------------------------------\n",
      "Maximum consecutive missing values: 2\n",
      "‚úÖ Missing pattern suitable for mean/median imputation\n",
      "\n",
      "üìä SIMPLE MEDIAN IMPUTATION\n",
      "========================================\n",
      "  BTC: filled with median 44959.47\n",
      "  ETH: filled with median 4007.06\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T09:33:57.548370Z",
     "start_time": "2025-09-21T09:33:57.533067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(f\"Mean comparison between before and after imputations data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "mean_val_missing = df_missing.mean()\n",
    "mean_val_full = simple_median_imputation.mean()\n",
    "\n",
    "# ŒëœÄŒøŒ∏ŒÆŒ∫ŒµœÖœÉŒ∑ œÉŒµ DataFrame\n",
    "comparison_mean = pd.DataFrame({\n",
    "    'Before': mean_val_missing,\n",
    "    'After': mean_val_full\n",
    "})\n",
    "\n",
    "print(comparison_mean.round(2))"
   ],
   "id": "fac9846314e8b5f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mean comparison between before and after imputations data\n",
      "==================================================\n",
      "        Before      After\n",
      "BTC  221978.94  171388.10\n",
      "ETH    4333.98    4240.55\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T09:33:57.596739Z",
     "start_time": "2025-09-21T09:33:57.582201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Standard deviation comparison between before and after imputations data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Œ•œÄŒøŒªŒøŒ≥ŒπœÉŒºœåœÇ œÑœÖœÄŒπŒ∫ŒÆœÇ Œ±œÄœåŒ∫ŒªŒπœÉŒ∑œÇ œÄœÅŒπŒΩ Œ∫Œ±Œπ ŒºŒµœÑŒ¨ œÑŒø imputation\n",
    "std_val_missing = df_missing.std()  # Œ†œÅŒπŒΩ œÑŒ∑ŒΩ ŒµœÄŒµŒæŒµœÅŒ≥Œ±œÉŒØŒ± œÑœâŒΩ NaN\n",
    "std_val_full = simple_median_imputation.std()  # ŒúŒµœÑŒ¨ œÑŒ∑ŒΩ Œ±ŒΩœÑŒπŒ∫Œ±œÑŒ¨œÉœÑŒ±œÉŒ∑ œÑœâŒΩ NaN\n",
    "\n",
    "# ŒëœÄŒøŒ∏ŒÆŒ∫ŒµœÖœÉŒ∑ œÉŒµ DataFrame\n",
    "comparison_std = pd.DataFrame({\n",
    "    'Before': std_val_missing,\n",
    "    'After': std_val_full\n",
    "})\n",
    "\n",
    "# ŒïŒ∫œÑœçœÄœâœÉŒ∑ œÉœÑœÅŒøŒ≥Œ≥œÖŒªŒøœÄŒøŒπŒ∑ŒºŒ≠ŒΩŒ∑ œÉŒµ 2 Œ¥ŒµŒ∫Œ±Œ¥ŒπŒ∫Œ¨\n",
    "print(comparison_std.round(2))"
   ],
   "id": "327db666b4afcbf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Standard deviation comparison between before and after imputations data\n",
      "==================================================\n",
      "        Before      After\n",
      "BTC  437097.60  377935.65\n",
      "ETH    2325.04    1970.34\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T09:33:57.645439Z",
     "start_time": "2025-09-21T09:33:57.634593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"=\" * 50)\n",
    "print(f\"Problem 2: Rolling Window for Missing Data \")\n",
    "print(\"=\" * 50)\n",
    "print(\"=\" * 50)"
   ],
   "id": "fbd9d2809b62137b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "==================================================\n",
      "Problem 2: Rolling Window for Missing Data \n",
      "==================================================\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T09:41:20.426767Z",
     "start_time": "2025-09-21T09:41:20.336676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 50)\n",
    "print(f\"Test window sizes: 3, 7, 14, 20, 30, 60 days\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# ŒõŒØœÉœÑŒ± ŒºŒµ Œ¥ŒπŒ±œÜŒøœÅŒµœÑŒπŒ∫Œ¨ ŒºŒµŒ≥Œ≠Œ∏Œ∑ œÄŒ±œÅŒ±Œ∏œçœÅŒøœÖ\n",
    "window_values = [3, 7, 14, 20, 30, 60]\n",
    "\n",
    "# ŒëœÄŒøŒ∏ŒÆŒ∫ŒµœÖœÉŒ∑ Œ±œÄŒøœÑŒµŒªŒµœÉŒºŒ¨œÑœâŒΩ œÉŒµ ŒªŒµŒæŒπŒ∫œå\n",
    "rolling_median_imputions = {}\n",
    "\n",
    "best_mae = float('inf')\n",
    "best_rmse = float('inf')\n",
    "for window in window_values:\n",
    "    print(f\"\\nüîπ Rolling Median Imputation with window = {window}\")\n",
    "    rolling_median = analyzer.rolling_median_imputation(window=window)\n",
    "    evalution = analyzer.evaluate_imputation_quality('rolling_median_imputation', rolling_median)\n",
    "    rolling_median_imputions[window] = rolling_median\n",
    "\n",
    "    mae_value = evalution['MAE']\n",
    "    rse_value = evalution['RMSE']\n",
    "\n",
    "    if mae_value < best_mae:\n",
    "        best_mae = mae_value\n",
    "        best_window_mae = window\n",
    "\n",
    "    if rse_value < best_rmse:\n",
    "        best_rmse = rse_value\n",
    "        best_window_rmse = window\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖChoose the best window size\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nBest MAE: {best_mae} , Window: {best_window_mae} \")\n",
    "print(f\"\\nBest RMSE: {best_rmse} , Window: {best_window_rmse} \")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "76f0467dba2ddb14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Test window sizes: 3, 7, 14, 20, 30, 60 days\n",
      "==================================================\n",
      "\n",
      "üîπ Rolling Median Imputation with window = 3\n",
      "\n",
      "üìä ROLLING MEDIAN IMPUTATION (Window: 3)\n",
      "==================================================\n",
      "  BTC: filled 1044 values with rolling median\n",
      "  ETH: filled 1044 values with rolling median\n",
      "\n",
      "üìà IMPUTATION QUALITY EVALUATION: ROLLING_MEDIAN_IMPUTATION\n",
      "============================================================\n",
      "  Data shapes - Missing: (3653, 2), Original: (3653, 2), Imputed: (3653, 2)\n",
      "  Found 2088 missing values to evaluate\n",
      "  Extracted 2088 value pairs\n",
      "  MAE: 5421.4562\n",
      "  RMSE: 20202.8587\n",
      "  MAPE: 4.10%\n",
      "  Correlation: 0.9981\n",
      "\n",
      "üí∞ FINANCIAL INTERPRETATION:\n",
      "  Average price difference: $5421.46\n",
      "  Typical error magnitude: $20202.86\n",
      "  ‚úÖ EXCELLENT: Very accurate imputation\n",
      "\n",
      "üîπ Rolling Median Imputation with window = 7\n",
      "\n",
      "üìä ROLLING MEDIAN IMPUTATION (Window: 7)\n",
      "==================================================\n",
      "  BTC: filled 1044 values with rolling median\n",
      "  ETH: filled 1044 values with rolling median\n",
      "\n",
      "üìà IMPUTATION QUALITY EVALUATION: ROLLING_MEDIAN_IMPUTATION\n",
      "============================================================\n",
      "  Data shapes - Missing: (3653, 2), Original: (3653, 2), Imputed: (3653, 2)\n",
      "  Found 2088 missing values to evaluate\n",
      "  Extracted 2088 value pairs\n",
      "  MAE: 5697.9882\n",
      "  RMSE: 22147.8790\n",
      "  MAPE: 4.13%\n",
      "  Correlation: 0.9978\n",
      "\n",
      "üí∞ FINANCIAL INTERPRETATION:\n",
      "  Average price difference: $5697.99\n",
      "  Typical error magnitude: $22147.88\n",
      "  ‚úÖ EXCELLENT: Very accurate imputation\n",
      "\n",
      "üîπ Rolling Median Imputation with window = 14\n",
      "\n",
      "üìä ROLLING MEDIAN IMPUTATION (Window: 14)\n",
      "==================================================\n",
      "  BTC: filled 1044 values with rolling median\n",
      "  ETH: filled 1044 values with rolling median\n",
      "\n",
      "üìà IMPUTATION QUALITY EVALUATION: ROLLING_MEDIAN_IMPUTATION\n",
      "============================================================\n",
      "  Data shapes - Missing: (3653, 2), Original: (3653, 2), Imputed: (3653, 2)\n",
      "  Found 2088 missing values to evaluate\n",
      "  Extracted 2088 value pairs\n",
      "  MAE: 6359.3599\n",
      "  RMSE: 25543.2827\n",
      "  MAPE: 4.55%\n",
      "  Correlation: 0.9971\n",
      "\n",
      "üí∞ FINANCIAL INTERPRETATION:\n",
      "  Average price difference: $6359.36\n",
      "  Typical error magnitude: $25543.28\n",
      "  ‚úÖ EXCELLENT: Very accurate imputation\n",
      "\n",
      "üîπ Rolling Median Imputation with window = 20\n",
      "\n",
      "üìä ROLLING MEDIAN IMPUTATION (Window: 20)\n",
      "==================================================\n",
      "  BTC: filled 1044 values with rolling median\n",
      "  ETH: filled 1044 values with rolling median\n",
      "\n",
      "üìà IMPUTATION QUALITY EVALUATION: ROLLING_MEDIAN_IMPUTATION\n",
      "============================================================\n",
      "  Data shapes - Missing: (3653, 2), Original: (3653, 2), Imputed: (3653, 2)\n",
      "  Found 2088 missing values to evaluate\n",
      "  Extracted 2088 value pairs\n",
      "  MAE: 7503.6305\n",
      "  RMSE: 30994.2660\n",
      "  MAPE: 5.21%\n",
      "  Correlation: 0.9958\n",
      "\n",
      "üí∞ FINANCIAL INTERPRETATION:\n",
      "  Average price difference: $7503.63\n",
      "  Typical error magnitude: $30994.27\n",
      "  ‚úÖ GOOD: Acceptable imputation quality\n",
      "\n",
      "üîπ Rolling Median Imputation with window = 30\n",
      "\n",
      "üìä ROLLING MEDIAN IMPUTATION (Window: 30)\n",
      "==================================================\n",
      "  BTC: filled 1044 values with rolling median\n",
      "  ETH: filled 1044 values with rolling median\n",
      "\n",
      "üìà IMPUTATION QUALITY EVALUATION: ROLLING_MEDIAN_IMPUTATION\n",
      "============================================================\n",
      "  Data shapes - Missing: (3653, 2), Original: (3653, 2), Imputed: (3653, 2)\n",
      "  Found 2088 missing values to evaluate\n",
      "  Extracted 2088 value pairs\n",
      "  MAE: 8956.0271\n",
      "  RMSE: 40406.5705\n",
      "  MAPE: 5.83%\n",
      "  Correlation: 0.9930\n",
      "\n",
      "üí∞ FINANCIAL INTERPRETATION:\n",
      "  Average price difference: $8956.03\n",
      "  Typical error magnitude: $40406.57\n",
      "  ‚úÖ GOOD: Acceptable imputation quality\n",
      "\n",
      "üîπ Rolling Median Imputation with window = 60\n",
      "\n",
      "üìä ROLLING MEDIAN IMPUTATION (Window: 60)\n",
      "==================================================\n",
      "  BTC: filled 1044 values with rolling median\n",
      "  ETH: filled 1044 values with rolling median\n",
      "\n",
      "üìà IMPUTATION QUALITY EVALUATION: ROLLING_MEDIAN_IMPUTATION\n",
      "============================================================\n",
      "  Data shapes - Missing: (3653, 2), Original: (3653, 2), Imputed: (3653, 2)\n",
      "  Found 2088 missing values to evaluate\n",
      "  Extracted 2088 value pairs\n",
      "  MAE: 12131.7898\n",
      "  RMSE: 58631.0232\n",
      "  MAPE: 7.87%\n",
      "  Correlation: 0.9852\n",
      "\n",
      "üí∞ FINANCIAL INTERPRETATION:\n",
      "  Average price difference: $12131.79\n",
      "  Typical error magnitude: $58631.02\n",
      "  ‚úÖ GOOD: Acceptable imputation quality\n",
      "==================================================\n",
      "‚úÖChoose the best window size\n",
      "--------------------------------------------------\n",
      "\n",
      "Best MAE: 5421.4562260536395 , Window: 3 \n",
      "\n",
      "Best RMSE: 20202.858669869875 , Window: 3 \n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 152
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
